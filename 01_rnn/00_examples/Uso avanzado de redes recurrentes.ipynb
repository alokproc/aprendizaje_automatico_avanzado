{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso avanzado de redes recurrentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección revisaremos tres técnicas avanzadas para mejorar el rendimiento y la capacidad de generalización de las redes neuronales recurrentes. Al final de la sección, sabremos la mayor parte de lo que se puede saber acerca del uso de RNNs en Keras.\n",
    "\n",
    "Ilustraremos las técnicas mediante un problema de predicción meteorológica, donde tendremos acceso a datos de series temporales provenientes de sensores instalados en el tejado de un edificio. Serán sensores de temperatura, presión atmosférica o humedad, que usaremos para predecir la temperatura 24 horas después de la recolección del último dato. Este problema es muy complejo y nos servirá para ilustrar muchas dificultades comunes que nos encontraremos al trabajar con time series.\n",
    "\n",
    "Cubriremos las siguientes técnicas:\n",
    "- ***Recurrent dropout***, una forma interna específica de utilizar *dropout* para tratar de evitar el effecto del *overfitting*.\n",
    "- ***Stacking recurrent layers***, para incrementar la representatividad de la red (a costa de increemntar el coste computacional).\n",
    "- ***Bidirectional recurrent layers***, que presentan la misma información a la RNN de formas diferentes, incrementando el *accuracy* y mitigando problemas de olvidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un problema de predicción meteorológica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora, los únicos datos secuenciales que habíamos visto estaban constituídos por texto, e.g. el dataset IMDB. Sin embargo, los datos secuenciales aparecen en muchas otros problemas aparte del procesamiento del lenguaje.\n",
    "\n",
    "En los ejemplos de esta sección trabajaremos con un dataset de timeseries con datos meteorológicos recogidos en la estación meteorológica situada en el *Max-Plank-Institute for Biogeochemestry* en Jena, Alemania: http://www.bgc-jena.mpg.de/wetter/.\n",
    "\n",
    "En este dataset se recogen catorce medidas diferentes, como temperatura, presión atmosférica, humedad, dirección del viento, etc., medidas cada diez minutos durante varios años. El dataset original llega hasta 2003, pero nosotros nos limitaremos al periodo 2009-2016.\n",
    "\n",
    "Este dataset es perfecto para trabajar con timeseries numéricas. Lo utilizaremos para, a partir de algunos datos del pasado reciente (unos pocos días), predecir la temperatura ambiente 24 horas después.\n",
    "\n",
    "Echemos un ojo a los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '../data/'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "\n",
    "print(header)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos las 420,552 lineas de datos en un Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, aquí vemos la gráfica de la temperatura (en Celsius) a lo largo del tiempo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "temp = float_data[:, 1]  # temperature (in degrees Celsius)\n",
    "plt.plot(range(len(temp)), temp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aprecia claramente la periodicidad anual de la temperatura.\n",
    "\n",
    "Aquí tenemos una gráfica más concreta, correspondiente a los diez primeros días (como se recogen datos cada diez minutos, se obtienen 144 datos por día):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1440), temp[:1440])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aprecia una cierta periodicidad diaria bastante evidente en los últimos cuatro días. Además, estos días se localizan en un mes frío de invierno.\n",
    "\n",
    "Una predicción para un mes, dados los datos de meses pasados parece asequible dada la periodicidad anual aparente. Sin embargo, en una escala de días la temperatura parece ser algo caótica.\n",
    "\n",
    "¿Será esta serie temporal predecible en una escala de días? Averigüémoslo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La formulación exacta de nuestro problema será la siguiente: dados datos que se remontan en el pasado *lookback timesteps* (un *timestep* es la periodicidad de nuestros datos, i.e. diez minutos) y que muestreamos cada *steps timesteps*, ¿podemos predecir la temperatura en *delay timesteps*?\n",
    "\n",
    "Utilizamos los siguientes parámetros:\n",
    "- lookback = 720, i.e. vamos hacia atrás 5 días.\n",
    "- steps = 6, i.e. muestreamos un dato por hora.\n",
    "- delay = 144, i.e. nuestro *target* estará 24 horas en el futuro.\n",
    "\n",
    "Para empezar necesitaremos hacer dos cosas:\n",
    "- Preprocesado de los datos de modo que una red neuronal pueda ingerirlos. Será sencillo ya que los datos son numéricos, luego no precisaremos realizar vectorización alguna. Sin embargo, cada timeserie en los datos viene en una escala diferente (e.g. la temperatura estará entre -20 y +30 normalmente, pero la presión, medida en milibares, estará alrededor de 100). Por tanto, normalizaremos cada timeserie por separado, de modo que todas tomen valores pequeños en una escala similar.\n",
    "- Crear un generador que tome el array de datos *float* y genere lotes de datos del pasado reciente, así como la temperatura a futuro como *target* asociado. Como las muestras en nuestro dataset serán altamente redundantes (por ejemplo, las muestras N y N+1 tendrán la mayoría de timesteps comunes), sería un malgasto de recursos el situar explícitamente cada muestra. En vez de eso, generaremos las muestras *on-the-fly* utilizando los datos originales.\n",
    "\n",
    "Preprocesamos los datos restando la media y dividiendo por la desviación típica cada timeserie. Utilizaremos los primeros 200,000 timesteps como datos de entrenamiento, así que calculamos la media y desviación típica sólo para esa fracción de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el *data generator* que utilizaremos. Devuelve una dupla ***(samples, targets)*** donde ***samples*** es un lote de *inputs* y ***targets*** es el correspondiente vector de *targets* con las temperaturas ambiente futuras a predecir. Toma los siguientes argumentos:\n",
    "- data: Los datos originales en forma de matriz de datos en formato de coma flotante, ya normalizados previamente.\n",
    "- lookback: Número de *timesteps* que nuestros datos de entrada irán hacia atrás.\n",
    "- delay: Número de *timesteps* en el futuro en el que predeciremos el *target*.\n",
    "- min_index y max_index: índices en la matriz que delimitan los *timesteps* de los que tomar los datos. Nos será útil para poder separar datos tanto para validación como para test.\n",
    "- shuffle: Si mezclamos nuestros datos o los consideramos en orden cronológico.\n",
    "- batch_size: número de muestras por lote.\n",
    "- step: Periodo, en *timesteps*, en el que muestreamos los datos. Lo fijaremos en 6 para tomar un dato cada hora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, usaremos nuestro generador abstracto para inicializar tres generadores (uno para *training*, otro para *validation* y el último para *test*). Cada uno mirará en segmentos diferentes de los datos: el de *training* en los 200,000 primeros *timesteps*, el de *validation* en los 100,000 siguentes y el de *test* en el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step, \n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "# This is how many steps to draw from `val_gen`\n",
    "# in order to see the whole validation set:\n",
    "val_steps = (300000 - 200001 - lookback) // batch_size\n",
    "\n",
    "# This is how many steps to draw from `test_gen`\n",
    "# in order to see the whole test set:\n",
    "test_steps = (len(float_data) - 300001 - lookback) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline por sentido común (sin machine learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de lanzarnos a aplicar cajas negras deep learning para (intentar) resolver el problema, probemos un método simple de sentido común que establecerá un *baseline* a partir del cual buscar soluciones mejores que prueben la utilidad modelos de ML más avanzados. Este tipo de soluciones por sentido común son un buen punto de partida cuando (aún) no se conocen soluciones para el problema que tratamos.\n",
    "\n",
    "Un ejemplo clásico sería el predecir la clase mayoritaria siempre en problemas de clasificación binaria con datos desbalanceados, siendo la *null acuracy* el *baseline* a batir. Ningún método que sea incapaz de batirlo podrá considerarse útil. Sorprendentemente, en ocasiones puede resultar complicado hacerlo.\n",
    "\n",
    "En nuestro caso, el *approach* de sentido común sería el suponer periodicidad diaria y predecir que la temperatura en 24 horas será la misma que ahora. Lo evaluamos utilizando el *score* Mean Absolute Error (MAE), que sería igual a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('np.mean(np.abs(preds - targets))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_naive_method():\n",
    "    batch_maes = []\n",
    "    for step in range(val_steps):\n",
    "        samples, targets = next(val_gen)\n",
    "        preds = samples[:, -1, 1]\n",
    "        mae = np.mean(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "    print(np.mean(batch_maes))\n",
    "    \n",
    "evaluate_naive_method()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este valor de 0.29 no es directamente interpretable al haber estandarizado los datos. Se traduciría a un error absoluto medio de 0.29 * temperature_std, es decir 2.57˚C. Se trata de un error bastante grande que trataremos de mejorar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un approach básico de machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que tiene sentido el probar un primer método de sentido común, no tiene menos el probar algún modelo de ML simple y barato computacionalmente antes de lanzarse a entrenar RNNs complejas. De este modo podemos comprobar la necesidad o no de la incorporación de esa complejidad.\n",
    "\n",
    "Probamos un modelo fully connected sencillo con 2 capas densas, donde previamente haremos un aplanamiento. Nótese que no hay función de activación en la capa de salida (típico en problemas de regresión). Para obtener resultados comparables con el método de sentido común, usaremos MAE como *loss*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos las curvas de *loss* para entrenamiento y validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos de los valores del *loss* de validación están cerca del valor obtenido en nuestro no-learning baseline, pero sin consistencia. Aquí tenemos una prueba de la necesidad de esos baseline sencillos. No parece que vaya a ser fácil batir ese 0.29.\n",
    "\n",
    "Cabe preguntarse porqué nuestro modelo deep learning no es capaz de acercarse al resultado naïve. La respuesta es que la red densa no está buscando predecir de la misma manera. Pudiera ocurrir que siendo posible una predicción que da un *loss* de 0.29 (como hemos comprobado) no seamos capaces de dar con un paradigma y una configuración que alcancen esa precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un primer baseline recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer intento con una red neuronal no fue bien. El realizar un aplanamiento tiene como consecuencia el que se pierde la referencia de secuencia temporal de los datos. Ahora miraremos los datos como lo que son: una secuencia donde la causalidad y el orden importan. Probaremos un modelo recurrente, ya que explotará la secuenciación de los datos, a diferencia del modelo anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de utilizar capas LSTM, usaremos una capa GRU, creadas por Chao et al. en 2014. Las capas GRU (que proviene de *gated recurrent unit*) sigue el mismo principio que las capas LSTM pero son menos costosas computacionalmente a cambio de tener algo menos de capacidad de representación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Mucho mejor! Hemos superado significativamente el baseline. El nuevo *validation loss* de ~0.265 (antes de hacer sobreajuste) se traduce en un error absoluto medio de 2.35˚C frente a los 2.57˚C de partida. Es una buena ganancia pero probablemente aún tengamos margen de mejora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de recurrent dropout para paliar el overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El overfitting en el modelo anterior es obvio y ocurre muy temprano. Ya sabemos en que consiste el dropout (\"esconder\" un porcentaje de los datos a la red para que no pueda ajustarse a ellos al no verlos), pero cómo usarlo en redes recurrentes no es trivial. Es sabido que aplicarlo anter de un layer recurrente obstaculiza el aprendizaje más que regulariza.\n",
    "\n",
    "En 2015, Yarin Gal, como parte de su tesis doctoral en deep learning Bayesiano, determinó la forma adecuada de aplicar dropout en redes recurrentes: ha de aplicarse en cada timestep el mismo patrón de unidades ignoradas. Más aún, en capas como LSTM o GRU un patrón constante debe ser aplicado en las activaciones recurrentes (*recurrent dropout mask*). De este modo la propagación del error a través del tiempo es correcta al no ser corrompida por la aleatoriedad en cada loop.\n",
    "\n",
    "El propio Yarin Gal utilizó Keras en su estudio e incorporó sus ideas al mismo como developer. Todo layer recurrente en Keras dispone de dos parámetros relacionados con dropout: ***dropout***, float indicando el dropout rate para el layer de entrada y ***recurrent_dropout***, específico para las unidades recurrentes. Utilicemos ambas estrategias en nuestra red GRU. Como las redes regularizadas mediente dropout tardan más en converger, entrenamos la red el dobe de epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un gran resultado. No hay overfitting durante las primeras 30 epochs. No obstante, pese a que tenemos una mayor estabilidad, nuestro mejor score no mejora en exceso el mejor resultado visto hasta ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking de capas recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya no hay overfitting, parece ser que nuestra red no es capaz de afinar más (*performance bottleneck*). Es hora de incrementar la capacidad de nuestra red. En general es buena idea incrementar la capacidad de la red hasta que se produce un overfitting pese al uso de estrategias correctoras (como dropout).\n",
    "\n",
    "La forma usual de incrementar la capacidad es aumentar el número de neuronas en las capas o aumentar el número de capas. *Recurrent layer stacking* es una estrategia clásica de aumento de capacidad. Por ejemplo, actualmente Google translate es un stack de siete capasLSTM grandes, lo que es enorme.\n",
    "\n",
    "Para hacer un stack con capas recurrentes, las capas intermedias deben devolver la secuencia total de outputs (que es un tensor 3D) en lugar de su output en el último timestamp. Se hace eligiendo return_sequences=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.1,\n",
    "                     recurrent_dropout=0.5,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.1, \n",
    "                     recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miremos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que no hay una mejoría significativa.\n",
    "\n",
    "Podemos extraer dos conclusiones:\n",
    "- Como no sufrimos un overfitting severo podríamos incrementar sin miedo el tamaño de nuestros layers para tratar de mejorar el *validation loss*. Además el incremento en el coste computacional es despreciable\n",
    "- Como añadir una capa no ha ayudado mucho, cabe plantearse el disminuir el retorno de secuencias para incrementar la capacidad en ese punto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de RNNs bidireccionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La última técnica que veremos es el uso de RNNs bidireccionales. Una bidireccional es una variante común de las RNNs que suele funcional mejor que las RNNs usuales en algunas tareas, por ejemplo el procesamiento de lenguaje natural (NLP por *natural languaje processing*).\n",
    "\n",
    "Las RNNs tratan los datos secuencialmente. Un shuffle o una inversión del orden temporal hacen que las representaciones extraidas de la secuencia sean totalmente diferentes. Ese tratamiento secuencial es el que permite un buen resultado en un problema como el que nos atañe.\n",
    "\n",
    "Un RNN bidireccional explota la sensibilidad al orden de las RNNs. Consiste en dos RNNs normales (e.g. LSTM o GRU) cada una de las cuales usa la frecuencia en una dirección (cronológica y anticronológicamente), uniendo después sus representaciones. El recorrido en ambas direcciones permite identificar patrones que podrían ser ignorados por la red ordenada cronológicamente.\n",
    "\n",
    "Teniendo en cuenta que el orden cronológico utilizado en los modelos hasta ahora puede considerarse una decisión arbitraria, cabe preguntarse qué habría pasado con una RNN unidireccional con secuencias ordenadas anticronológicamente.\n",
    "\n",
    "Comprobémoslo. Para ello basta modificar el data generator cambiando su última línea por ***yield samples[:, ::-1, :], targets)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_order_generator(data, lookback, delay, min_index, max_index,\n",
    "                            shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples[:, ::-1, :], targets\n",
    "        \n",
    "train_gen_reverse = reverse_order_generator(\n",
    "    float_data,\n",
    "    lookback=lookback,\n",
    "    delay=delay,\n",
    "    min_index=0,\n",
    "    max_index=200000,\n",
    "    shuffle=True,\n",
    "    step=step, \n",
    "    batch_size=batch_size)\n",
    "val_gen_reverse = reverse_order_generator(\n",
    "    float_data,\n",
    "    lookback=lookback,\n",
    "    delay=delay,\n",
    "    min_index=200001,\n",
    "    max_index=300000,\n",
    "    step=step,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen_reverse,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen_reverse,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego el orden inverso perjudica seriamente al modelo, que está lejos incluso del modelo baseline inicial. Tiene sentido ya que el layer GRU tendrá más problemas para recordar la información más antigua (que es la más reciente en el anticronológico). \n",
    "\n",
    "Aunque parece un argumento lógico universal no es siempre trivialmente cierto. Por ejemplo, en NLP la importancia de una palabra para entender una frase no suele depender de la posición de la palabra en la misma. Probamos el mismo truco en la LSTM para los datos IMDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Number of words to consider as features\n",
    "max_features = 10000\n",
    "# Cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 500\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Reverse sequences\n",
    "x_train = [x[::-1] for x in x_train]\n",
    "x_test = [x[::-1] for x in x_test]\n",
    "\n",
    "# Pad sequences\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128))\n",
    "model.add(layers.LSTM(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es casi idéntico con ambas ordenaciones, lo que indica que el orden de las palabras en la frase contribuye al entendimiento en mucha menor medida que las palabras en sí. No obstante, en ambos casos se aprenderían representaciones diferentes, lo que hace que merezca la pena el doble esfuerzo. Además sería posible el plantearse estrategias de aprovechamiento conjunto, como *ensemble learning*.\n",
    "\n",
    "Una RNN bidireccional explota la idea de mejorar el resultado de la RNN cronológica considerando las representaciones obtenidas con ambas ordenaciones conjuntamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display\n",
    "display(Image(filename='BidirectionalRNN.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para inicializar una RNN bidireccional en Keras se usa una capa ***Bidirectional***, que toma como primer argumento una capa recurrente. Se creará una segunda instancia en orden anticronológico separada de modo que una procesa las secuencias de entrada en un orden y la otra en el inverso.\n",
    "\n",
    "Lo probamos en el problema de *sentiment analysis* con IMDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciona un poco mejor que la LSTM básica, llegando a un 88% de validation accuracy. Parece que hay algo de overfitting, lo que es lógico al tener el doble de parámetros que la LSTM cronológica. Con algo de regularización la bidireccional será muy competitiva en este dataset.\n",
    "\n",
    "Probemos ahora el el problema de predicción meteorológica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(\n",
    "    layers.GRU(32), input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciona casi igual que el GRU cronológico. Tiene sentido ya que parece que toda la capacidad predictiva está en la mitad cronológica de la bidireccional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yendo más allá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aún hay muchas otras cosas que podríamos hacer para tratar de mejorar nuestro modelo:\n",
    "- Ajustar el número de neuronas en cada layer recurrente al hacer stack. Nuestras elecciones han sido arbitrarias, luego potencialmente subóptimas.\n",
    "- Ajustar el learning rate del optimizer.\n",
    "- Usar layers LSTM en lugar de GRU.\n",
    "- Usar un dense layer mayor o con más capas.\n",
    "- No olvidar probar eventualmente los modelos en el test set separado a tal efecto (en términos del MAE).\n",
    "\n",
    "Todo problema es único y habrá que probar y evaluar diferentes estrategias empíricamente. Probar e iterar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
