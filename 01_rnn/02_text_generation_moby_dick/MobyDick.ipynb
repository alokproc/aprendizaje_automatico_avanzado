{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M4n_92HtC8lf"
   },
   "source": [
    "# Generación de texto con LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qJVMD08C8lm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: importar librerías\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_pMK7n-C8l1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1238244"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Paso 2: importar cuento  + pasar todo a minusculas + pasar caracteres a integers\n",
    "# load ascii text and covert to lowercase\n",
    "f = open('MobyDick.txt', 'r', encoding=\"utf8\") \n",
    "text = f.read()\n",
    "f.close()\n",
    "text = text.lower()\n",
    "#text = re.sub(r\"[^a-zA-Z0-9]+\", ' ', text)\n",
    "#text = text[:100000]\n",
    "\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = np.array(list(text))\n",
    "encoder = preprocessing.LabelEncoder().fit(chars)\n",
    "encoded_chars = encoder.transform(chars)\n",
    "\n",
    "# normalize data\n",
    "mean = encoded_chars.mean()\n",
    "stdv = encoded_chars.std()\n",
    "encoded_chars = (encoded_chars - mean) / stdv\n",
    "\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyvuR3l7C8l-"
   },
   "outputs": [],
   "source": [
    "#paso 3: preparar datos: \n",
    "# 1- preprocesar: ejemplo: CHAPT -> E; HAPTE -> R (pero con 100 caracteres)\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps, n_step_diff=1):\n",
    "    X, y = list(), list()\n",
    "    i = 0\n",
    "    while i <len(sequence): \n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        i = i + (n_step_diff)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = split_sequence(encoded_chars, 100, 100)\n",
    "\n",
    "# 2- cambiar dataset para que la entrada sea válida: [Samples, time_steps,features]\n",
    "n_samples = X_train.shape[0]\n",
    "n_steps = X_train.shape[1] # 100\n",
    "n_features = 1\n",
    "X_train = X_train.reshape((n_samples, n_steps, n_features))\n",
    "\n",
    "# 3-Normalizar\n",
    "    # normalizamos todo después de usar label encoder\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IBU8GojbC8mP"
   },
   "outputs": [],
   "source": [
    "# Definimos modelo LSTM\n",
    "lstm_model = models.Sequential([\n",
    "    layers.LSTM(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, input_shape=X_train.shape[-2:]),\n",
    "    layers.LSTM(32, dropout=0.1, recurrent_dropout=0.5),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='rmsprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_iZiLTxXC8mX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9905 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9905/9905 [==============================] - 42s 4ms/step - loss: 0.9562 - val_loss: 0.9709\n",
      "Epoch 2/5\n",
      "9905/9905 [==============================] - 40s 4ms/step - loss: 0.9475 - val_loss: 0.9569\n",
      "Epoch 3/5\n",
      "9905/9905 [==============================] - 40s 4ms/step - loss: 0.9390 - val_loss: 0.9551\n",
      "Epoch 4/5\n",
      "9905/9905 [==============================] - 40s 4ms/step - loss: 0.9390 - val_loss: 0.9548\n",
      "Epoch 5/5\n",
      "9905/9905 [==============================] - 41s 4ms/step - loss: 0.9377 - val_loss: 0.9529\n"
     ]
    }
   ],
   "source": [
    "#Hacemos el FIT\n",
    "history = lstm_model.fit(X_train, y_train, epochs=5, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "MobyDick.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
