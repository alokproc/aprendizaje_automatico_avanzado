{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KM0Y-uAVb4UD"
   },
   "source": [
    "# exercise: sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOjaMdKwb4UJ"
   },
   "source": [
    "Binary classification is possibly the most common type of problem in ML applications. In this one we're going to see how to build a classifier in Keras. Specifically, we will build a neural network to help us classify movie opinions as \"positive/negative\" based only on the textual content of the movie. It will be an example of what is currently known as sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9ykCQdDb4UO"
   },
   "source": [
    "## IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD3UxgoRb4US"
   },
   "source": [
    "We will use one of the datasets provided by the IMDB site (Internet Movie DataBase) formed by 50,000 highly polarized opinions (which simplifies the learning task). This set is divided in 50%/50% for training/test, and in each of them there are 50% of opinions of each type.\n",
    "\n",
    "As with the previous dataset (MNIST) this dataset also comes with Keras, and in addition is provided pre-processed: by means of an indexed dictionary, the opinions (word sequences) have been converted into sequences of integers.\n",
    "\n",
    "The code that loads the dataset (about 80Mb of data that will be downloaded the first time it is executed) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10062,
     "status": "ok",
     "timestamp": 1584999760848,
     "user": {
      "displayName": "Ekhi Zugasti Uriguen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjLW21F9JMtJwfUDFcY2ROtJC3l1QPlGpvoN9Qy=s64",
      "userId": "00195558535022081217"
     },
     "user_tz": -60
    },
    "id": "DbN_nV4ib4UV",
    "outputId": "253ba047-54a1-4c40-cc70-f860122ebcd2"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "numwords=10000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=numwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1sgD_whb4Ui"
   },
   "source": [
    "As the dictionary is tremendously large and there are many words that are rarely used, we'll be left with only the 10,000 most frequent words (something we get by means of the argument num_words = 10000 in the loading process).\n",
    "\n",
    "If you wish, you can explore the content of the train_data and test_data variables, which are lists of opinions, where each opinion is a list of integers (encoding a sequence of words). The variables train_labels and test_labels are binary lists, where 0 indicates that the associated opinion is negative and 1 that it is positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 4, 2, 2, 33, 2804, 4, 2040, 432, 111, 153,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, ...\n",
       "1  [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463,...\n",
       "2  [1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5...\n",
       "3  [1, 4, 2, 2, 33, 2804, 4, 2040, 432, 111, 153,...\n",
       "4  [1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 1..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(train_data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  0\n",
       "2  0\n",
       "3  1\n",
       "4  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_labels).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FsS54CABb4Ul"
   },
   "source": [
    "\n",
    "\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMhzTRi5b4Un"
   },
   "source": [
    "Since neural networks do not support lists of integers as input, because they are of variable length, we have to add additional pre-processing to be able to work with them. We have two options:\n",
    "\n",
    "- Complete the shorter lists so that they're all the same length, and then convert them into tensors that will feed the input layer of the network.\n",
    "- Encode the lists in One-hot to convert them into 0s and 1s vectors. Since we have a maximum of 10,000 words in our vocabulary, each opinion will be converted into a binary list of 10,000 positions indicating which words appear in the opinion. In this case, the first (dense) layer of our network would be connected with vectors of length 10,000.\n",
    "\n",
    "We will choose this second option, which over time we will see has more advantages than the first.\n",
    "\n",
    "The code that allows us to do this conversion is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fImSfZXwb4Uq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=numwords):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yK1yZ8kb4Uy"
   },
   "source": [
    "We would also have to do a conversion with the labels, but since in this case they are already binary vectors, it is enough to convert them into numerical ones:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tE7vdcRxb4U0"
   },
   "outputs": [],
   "source": [
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bo0OEMd3b4U8"
   },
   "source": [
    "## Network Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_Rm5lq3b4U-"
   },
   "source": [
    "As we have seen, the input data are vectors, and the labels are scalar (1s and 0s), so this is the simplest configuration possible to be worked with a neural network, and a type of network that works well with this type of problem is a simple stack of dense layers with relu activations, which in Keras are built with the instruction: layers.Dense(16, activation='relu').\n",
    "\n",
    "In general, in all types of layers, the most common arguments you will have to use will be the number of neurons in the layer, and the type of activation these neurons will use. In the previous case we used 16 neurons, which means that this layer will use 16 dimensions to try to structure the patterns it finds in the input data according to the target function (loss) that it should optimize.\n",
    "\n",
    "It can be intuitively interpreted that the dimension of the layer represents how much freedom the network is allowed to learn internal representations. Having more units allows you to learn more complex representations, but also increases the computational load and facilitates the memorization of patterns in the training data (which may not be relevant to the problem and may lead to an over-adjustment phenomenon).\n",
    "\n",
    "Regarding the architecture when working with dense layers, there are two key decisions to consider:\n",
    "\n",
    "\n",
    "*   How many layers to use.\n",
    "\n",
    "*   How many units to place in each layer.\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "While there are no general rules for how to make these decisions, there are some reasons that can be learned from experience and some implicit knowledge useful for network design. For this example, without a clear justification, and only as a demonstration of the techniques we will use three intermediate layers of 16 neurons each, and a third layer that will have a single scalar output (which will represent the model's prediction). The intermediate layers will use relu as an activation function, and the final layer will use a sigmoid (which has an output in [0, 1]).\n",
    "\n",
    "The implementation in Keras, similar to the one we already did for MNIST, is therefore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1585003968093,
     "user": {
      "displayName": "Ekhi Zugasti Uriguen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjLW21F9JMtJwfUDFcY2ROtJC3l1QPlGpvoN9Qy=s64",
      "userId": "00195558535022081217"
     },
     "user_tz": -60
    },
    "id": "zr8Sk4dob4VA",
    "outputId": "7cc74137-948e-4eaa-944a-086a726176f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 4)                 40004     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 40,049\n",
      "Trainable params: 40,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "red = models.Sequential()\n",
    "red.add(layers.Dense(4, activation='relu', input_shape=(numwords,)))\n",
    "red.add(layers.Dense(4, activation='relu'))\n",
    "red.add(layers.Dense(4, activation='relu'))\n",
    "red.add(layers.Dense(1, activation='sigmoid'))\n",
    "red.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14561,
     "status": "ok",
     "timestamp": 1584999765392,
     "user": {
      "displayName": "Ekhi Zugasti Uriguen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjLW21F9JMtJwfUDFcY2ROtJC3l1QPlGpvoN9Qy=s64",
      "userId": "00195558535022081217"
     },
     "user_tz": -60
    },
    "id": "Re9jQl4eOOTC",
    "outputId": "c9e9b172-4a64-4923-d17c-3aabad741b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(red, to_file='IMDBModel_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqi5crYUb4VF"
   },
   "source": [
    "Following the same pattern we saw in the previous example, we need to choose the loss function (which will be minimized) and the optimization method (which will seek to minimize that function).\n",
    "\n",
    "Since we are facing a binary classification problem and the output of our network is a probability (provided by the sigmoid), we will use binary_crossentropy as the loss function. This is not the only viable option, we could have chosen, for example, mean_squared_error, but in this case binary_crossentropy is a better option because we are working with probabilities.\n",
    "\n",
    "Cross entropy comes from the Information Theory field, and measures the distance between probability distributions (in this case, the distribution calculated by the predictor and the one that represents the actual distribution coming from the training data).\n",
    "\n",
    "As an optimizer we will use rmsprop, which is usually a good choice in almost all cases. To monitor the evolution of learning we will use only one metric, accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gecGwuQZb4VH"
   },
   "outputs": [],
   "source": [
    "red.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6eoQENnpb4VO"
   },
   "source": [
    "In this case we are passing all the data as strings that come predefined in Keras and work with fixed parameters, but it is also possible to adjust with more flexibility each one of them and to configure the parameters on which it depends, and even to pass you functions, either those that Keras brings or completely customized. For example:\n",
    " - we can set a new learning Rate. The lower the learning rate, the more difficult will be achieving the optimal point (so we would need to add more epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CunMAWLb4VP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "red.compile(optimizer=optimizers.RMSprop(lr=0.0001), #the default value is 0.001\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "heMNn39Bb4VV"
   },
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n7vFSm_Tb4VX"
   },
   "source": [
    "n order to monitor the metrics (accuracy) while training is taking place we need to have more data that the model does not use during the process. We will create a validation set by separating another 10,000 samples from the original set. So the situation is as follows:\n",
    "- Training set: with which we will try to optimize the weights of the network so that it minimizes the loss function. In this process we use a model that depends on certain parameters that may need to be adjusted to achieve better performance.\n",
    "- Validation set: with which we will measure how good is the concrete model we are training (with fixed parameters). It is something like a temporary test set. It allows to adjust these parameters to improve the performance.\n",
    "- Test set: that has not been used at any moment of the previous iterations and that allows to measure in an objective way the goodness of the final model obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKPokIlvb4VZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "partial_x_train, x_val, partial_y_train, y_val = train_test_split(x_train, y_train, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "olPf5tWFb4Vg"
   },
   "source": [
    "We will then train the model on the complete training set (no validation, no test), in mini-batches of 512 samples (i.e. every 512 samples analyzed, the weights of the network are updated).\n",
    "\n",
    "We will monitor loss and accuracy over the 10,000 samples left in the validation set. To do this, we use the validation_data argument of the fit function, which we did not use in the previous example.\n",
    "\n",
    "As predicting the right number of iterations (epochs) can be complicated, we will prefix a high number of epochs (500) and define an early stop criterion using the EarlyStopping function (to which several decisions must be indicated, the most relevant being the monitoring criterion to be used and the number of epochs without improvement in it before stopping, called patience):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86949,
     "status": "ok",
     "timestamp": 1585004067930,
     "user": {
      "displayName": "Ekhi Zugasti Uriguen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjLW21F9JMtJwfUDFcY2ROtJC3l1QPlGpvoN9Qy=s64",
      "userId": "00195558535022081217"
     },
     "user_tz": -60
    },
    "id": "xE643n8bb4Vh",
    "outputId": "173dd82d-7c71-41ef-fc77-3d83744f509b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21250 samples, validate on 3750 samples\n",
      "Epoch 1/500\n",
      "  512/21250 [..............................] - ETA: 1:04WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " Blas GEMM launch failed : a.shape=(512, 10000), b.shape=(10000, 4), m=512, n=4, k=10000\n\t [[node sequential/dense/MatMul (defined at <ipython-input-12-eafb21550395>:14) ]] [Op:__inference_distributed_function_1097]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-eafb21550395>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                         validation_data=(x_val, y_val))\n\u001b[0m",
      "\u001b[1;32mZ:\\Programacion\\Anaconda\\envs\\deepl\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mZ:\\Programacion\\Anaconda\\envs\\deepl\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\Programacion\\Anaconda\\envs\\deepl\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\Programacion\\Anaconda\\envs\\deepl\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\Programacion\\Anaconda\\envs\\deepl\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\Programacion\\Anaconda\\envs\\deepl\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\Programacion\\Anaconda\\envs\\deepl\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\Programacion\\Anaconda\\envs\\deepl\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\Programacion\\Anaconda\\envs\\deepl\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mZ:\\Programacion\\Anaconda\\envs\\deepl\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mZ:\\Programacion\\Anaconda\\envs\\deepl\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  Blas GEMM launch failed : a.shape=(512, 10000), b.shape=(10000, 4), m=512, n=4, k=10000\n\t [[node sequential/dense/MatMul (defined at <ipython-input-12-eafb21550395>:14) ]] [Op:__inference_distributed_function_1097]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "EarlyStopping_CallBack = [EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0,\n",
    "                                        patience=5,\n",
    "                                        verbose=0,\n",
    "                                        mode='auto',\n",
    "                                        baseline=None)]\n",
    "entrenamiento = red.fit(partial_x_train,\n",
    "                        partial_y_train,\n",
    "                        callbacks=EarlyStopping_CallBack,\n",
    "                        epochs=500,\n",
    "                        batch_size=512,\n",
    "                        validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW5dK1kxb4Vm"
   },
   "source": [
    "the call to fit() returns a history object, which has the following structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1585004095919,
     "user": {
      "displayName": "Ekhi Zugasti Uriguen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjLW21F9JMtJwfUDFcY2ROtJC3l1QPlGpvoN9Qy=s64",
      "userId": "00195558535022081217"
     },
     "user_tz": -60
    },
    "id": "rEG63sQxb4Vo",
    "outputId": "b17ff3ff-2356-4b3a-9b3b-a778626c9023"
   },
   "outputs": [],
   "source": [
    "entrenamiento_dict = entrenamiento.history\n",
    "entrenamiento_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1Z4XVELb4Vs"
   },
   "source": [
    "In this case, this object contains 4 entries, one for each metric being monitored during training and during validation. We can use Matplotlib to represent the training and validation losses/accuracies simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1795,
     "status": "ok",
     "timestamp": 1585004248630,
     "user": {
      "displayName": "Ekhi Zugasti Uriguen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjLW21F9JMtJwfUDFcY2ROtJC3l1QPlGpvoN9Qy=s64",
      "userId": "00195558535022081217"
     },
     "user_tz": -60
    },
    "id": "IFBp0GiTb4Vv",
    "outputId": "63107b73-4b06-4c0b-ed46-f02be6aee24e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ent_acc = entrenamiento.history['binary_accuracy']\n",
    "val_acc = entrenamiento.history['val_binary_accuracy']\n",
    "ent_loss = entrenamiento.history['loss']\n",
    "val_loss = entrenamiento.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(ent_acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, ent_loss, 'bo', label='Training')\n",
    "# r* red star\n",
    "plt.plot(epochs, val_loss, 'r*', label='Validation')\n",
    "plt.title('Loss in Training and Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1585004284077,
     "user": {
      "displayName": "Ekhi Zugasti Uriguen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjLW21F9JMtJwfUDFcY2ROtJC3l1QPlGpvoN9Qy=s64",
      "userId": "00195558535022081217"
     },
     "user_tz": -60
    },
    "id": "5vS-2yhYb4Vz",
    "outputId": "98582733-5a4a-41b3-9d9b-25e60fc643a5"
   },
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "acc_values = entrenamiento_dict['binary_accuracy']\n",
    "val_acc_values = entrenamiento_dict['val_binary_accuracy']\n",
    "\n",
    "plt.plot(epochs, ent_acc, 'bo', label='Training')\n",
    "plt.plot(epochs, val_acc, 'r*', label='Validation')\n",
    "plt.title('Accuracy in Training and Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wjua5g21b4V5"
   },
   "source": [
    "These graphs show that the training loss decreases with each epoch, and that the training accuracy increases, which indicates that the optimization procedure is working properly (especially regarding the loss function). But in this case we observe that the same does not happen with the validation, which begin to worsen from the first epochs. This is a clear case of overfitting: after a few steps the system over-adjusts to the training data, and learns a representation that is specific to this data and cannot be generalised to other data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E75XoYSDb4V7"
   },
   "source": [
    "Lets try to evaluate the model on the test data, not used until now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4778,
     "status": "ok",
     "timestamp": 1585004379727,
     "user": {
      "displayName": "Ekhi Zugasti Uriguen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjLW21F9JMtJwfUDFcY2ROtJC3l1QPlGpvoN9Qy=s64",
      "userId": "00195558535022081217"
     },
     "user_tz": -60
    },
    "id": "O1TMRsOUb4V8",
    "outputId": "7747bbeb-e60f-4b3e-d693-517710517dd2"
   },
   "outputs": [],
   "source": [
    "results = red.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6sfYDk8b4V_"
   },
   "source": [
    "We see that the very simple approach we have made achieves a fairly high accuracy, although the current state of the art for this problem is around 95%, but working on much more elaborate networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Me0Q41WCb4WB"
   },
   "source": [
    "## Predicción sobre datos nuevos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXZdq8TAb4WE"
   },
   "source": [
    "Having trained the network, the natural step is to use the model for something practical. We can generate new predictions about opinions to analyze whether they are positive or not using the predict method associated with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2727,
     "status": "ok",
     "timestamp": 1585004490631,
     "user": {
      "displayName": "Ekhi Zugasti Uriguen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjLW21F9JMtJwfUDFcY2ROtJC3l1QPlGpvoN9Qy=s64",
      "userId": "00195558535022081217"
     },
     "user_tz": -60
    },
    "id": "hzqC7VjOb4WF",
    "outputId": "20553ce0-a6a0-4586-a00d-18493daa14a5"
   },
   "outputs": [],
   "source": [
    "red.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKiCP3-Wb4WK"
   },
   "source": [
    "We see that the model is very determining in some cases (reaching values like 0.99 or 0.01) but not so much in others (with values close to a medium, like 0.46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8sqwHKjb4WL"
   },
   "source": [
    "## Proposed Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTvTz-bIb4WN"
   },
   "source": [
    "Easy small changes:\n",
    "- In the previous model we used 3 hidden layers... check the effect of increasing or decreasing this number on validation and test accuracy.\n",
    "- Change the number of units in the hidden layers (8, 32, 64,...) and measure their effect.\n",
    "- See what happens when you use mse as a loss function, instead of binary_crossentropy?\n",
    "- See what happens when using tanh activation instead of relu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGl1pRykpGXy"
   },
   "source": [
    "Use Hyperas for Parameter tunning:\n",
    "Select a fixed number of layers and let Hyperas select the best hyperparameters for all of them \n",
    "\n",
    "\n",
    "*   Number of neurons in each layer\n",
    "*   Activation function for each layer\n",
    "* Dropout value \n",
    "* optimizer\n",
    "* Batch size\n",
    "\n",
    "Is it possible to add or remove layers with hyperas?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "znMP6jxSqosf"
   },
   "source": [
    "Hyperas is not currently installed in google colab, you can install it, executing the code:\n",
    "\n",
    "\n",
    "```\n",
    "!pip install hyperas\n",
    "```\n",
    "\n",
    "help for using Hyperas in colab: https://nilsschlueter.de/blog/articles/keras-hyperparameter-tuning-in-google-colab-using-hyperas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy small changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the previous model we used 3 hidden layers... check the effect of increasing or decreasing this number on validation and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we try adding more hidden layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "nn = models.Sequential([\n",
    "    layers.Dense(4, activation='relu', input_shape=(numwords,)),\n",
    "    layers.Dense(4, activation='relu'),\n",
    "    layers.Dense(4, activation='relu'),\n",
    "    layers.Dense(4, activation='relu'),\n",
    "    layers.Dense(4, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "nn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "EarlyStopping_CallBack = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)]\n",
    "nn.fit(partial_x_train, partial_y_train, callbacks=EarlyStopping_CallBack, epochs=500, batch_size=512, validation_data=(x_val, y_val), verbose=0)\n",
    "nn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we try decreasing hidden layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "nn = models.Sequential([\n",
    "    layers.Dense(4, activation='relu', input_shape=(numwords,)),\n",
    "    layers.Dense(4, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "nn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "EarlyStopping_CallBack = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)]\n",
    "nn.fit(partial_x_train, partial_y_train, callbacks=EarlyStopping_CallBack, epochs=500, batch_size=512, validation_data=(x_val, y_val), verbose=0)\n",
    "nn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the number of units in the hidden layers (8, 32, 64,...) and measure their effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "nn = models.Sequential([\n",
    "    layers.Dense(8, activation='relu', input_shape=(numwords,)),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "nn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "EarlyStopping_CallBack = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)]\n",
    "nn.fit(partial_x_train, partial_y_train, callbacks=EarlyStopping_CallBack, epochs=500, batch_size=512, validation_data=(x_val, y_val), verbose=0)\n",
    "nn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "nn = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(numwords,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "nn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "EarlyStopping_CallBack = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)]\n",
    "nn.fit(partial_x_train, partial_y_train, callbacks=EarlyStopping_CallBack, epochs=500, batch_size=512, validation_data=(x_val, y_val), verbose=0)\n",
    "nn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what happens when you use mse as a loss function, instead of binary_crossentropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "nn = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(numwords,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "nn.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['binary_accuracy'])\n",
    "\n",
    "EarlyStopping_CallBack = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)]\n",
    "nn.fit(partial_x_train, partial_y_train, callbacks=EarlyStopping_CallBack, epochs=500, batch_size=512, validation_data=(x_val, y_val), verbose=0)\n",
    "nn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what happens when using tanh activation instead of relu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "nn = models.Sequential([\n",
    "    layers.Dense(64, activation='tanh', input_shape=(numwords,)),\n",
    "    layers.Dense(64, activation='tanh'),\n",
    "    layers.Dense(64, activation='tanh'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "nn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "EarlyStopping_CallBack = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)]\n",
    "nn.fit(partial_x_train, partial_y_train, callbacks=EarlyStopping_CallBack, epochs=500, batch_size=512, validation_data=(x_val, y_val), verbose=0)\n",
    "nn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def data():\n",
    "    # we use directly global variables\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    nn = models.Sequential([\n",
    "        layers.Dense(4, activation='relu', input_shape=(numwords,)),\n",
    "        layers.Dense(4, activation='relu'),\n",
    "        layers.Dense(4, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    nn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    #EarlyStopping_CallBack = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)]\n",
    "    nn.fit(partial_x_train, partial_y_train, epochs=500, batch_size=512, validation_data=(x_val, y_val), verbose=0)    \n",
    "    score, acc = nn.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "\n",
    "best_run, best_model = optim.minimize(\n",
    "    model=model, \n",
    "    data=data, \n",
    "    max_evals=10, \n",
    "    algo=tpe.suggest, \n",
    "    notebook_name='SentimentAnalysisExample+exercise', \n",
    "    trials=Trials()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SentimentAnalysisExample+exercise.ipynb",
   "provenance": [
    {
     "file_id": "1ZL0FstT3_lcGoFIYk6ZqKBfdQIs7x1uf",
     "timestamp": 1585007864366
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
