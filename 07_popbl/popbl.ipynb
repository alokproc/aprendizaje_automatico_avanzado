{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  148574\n",
      "Total Vocab:  46\n"
     ]
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  148474\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 133626 samples, validate on 14848 samples\n",
      "Epoch 1/20\n",
      "133626/133626 [==============================] - 134s 999us/sample - loss: 2.9691 - val_loss: 2.8359\n",
      "Epoch 2/20\n",
      "133626/133626 [==============================] - 132s 987us/sample - loss: 2.7803 - val_loss: 2.7026\n",
      "Epoch 3/20\n",
      "133626/133626 [==============================] - 131s 982us/sample - loss: 2.6921 - val_loss: 2.6512\n",
      "Epoch 4/20\n",
      "133626/133626 [==============================] - 132s 987us/sample - loss: 2.6417 - val_loss: 2.6240\n",
      "Epoch 5/20\n",
      "133626/133626 [==============================] - 128s 961us/sample - loss: 2.6005 - val_loss: 2.5599\n",
      "Epoch 6/20\n",
      "133626/133626 [==============================] - 128s 956us/sample - loss: 2.5592 - val_loss: 2.5443\n",
      "Epoch 7/20\n",
      "133626/133626 [==============================] - 129s 964us/sample - loss: 2.6053 - val_loss: 2.5242\n",
      "Epoch 8/20\n",
      "133626/133626 [==============================] - 127s 948us/sample - loss: 2.5780 - val_loss: 2.5090\n",
      "Epoch 9/20\n",
      "133626/133626 [==============================] - 125s 936us/sample - loss: 2.5504 - val_loss: 2.5118\n",
      "Epoch 10/20\n",
      "133626/133626 [==============================] - 128s 954us/sample - loss: 2.4924 - val_loss: 2.4783\n",
      "Epoch 11/20\n",
      "133626/133626 [==============================] - 123s 922us/sample - loss: 2.4589 - val_loss: 2.4568\n",
      "Epoch 12/20\n",
      "133626/133626 [==============================] - 123s 920us/sample - loss: 2.4361 - val_loss: 2.4350\n",
      "Epoch 13/20\n",
      "133626/133626 [==============================] - 124s 926us/sample - loss: 2.4138 - val_loss: 2.4258\n",
      "Epoch 14/20\n",
      "133626/133626 [==============================] - 128s 958us/sample - loss: 2.3987 - val_loss: 2.4094\n",
      "Epoch 15/20\n",
      "133626/133626 [==============================] - 133s 998us/sample - loss: 2.3782 - val_loss: 2.4016\n",
      "Epoch 16/20\n",
      "133626/133626 [==============================] - 140s 1ms/sample - loss: 2.3687 - val_loss: 2.3840\n",
      "Epoch 17/20\n",
      "133626/133626 [==============================] - 149s 1ms/sample - loss: 2.3525 - val_loss: 2.3689\n",
      "Epoch 18/20\n",
      "133626/133626 [==============================] - 152s 1ms/sample - loss: 2.3601 - val_loss: 2.3680\n",
      "Epoch 19/20\n",
      "133626/133626 [==============================] - 145s 1ms/sample - loss: 2.3261 - val_loss: 2.3525\n",
      "Epoch 20/20\n",
      "133626/133626 [==============================] - 141s 1ms/sample - loss: 2.3089 - val_loss: 2.3329\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(256, dropout=0.2, recurrent_dropout=0.2, input_shape=(X.shape[1], X.shape[2])),\n",
    "    Dense(y.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# early stopping callback\n",
    "early_stop = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None)]\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X, y, epochs=20, batch_size=128, callbacks=early_stop, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/model_v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/model_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" nything more:  at\n",
      "last came a rumbling of little cartwheels, and the sound of a\n",
      "good many voices all \"\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " toe to tee soeer tee sooe to the sas oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo th\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
